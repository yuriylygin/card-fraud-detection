{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef757da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from src.data.initiate import initiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed07838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to generate customer profiles table: 0.089s\n",
      "Time to generate terminal profiles table: 0.086s\n",
      "Time to associate terminals to customers: 3.6s\n",
      "Time to generate transactions: 2.5e+02s\n"
     ]
    }
   ],
   "source": [
    "customer_profiles_table_df, terminal_profiles_table_df, transactions_df = initiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e7a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   CUSTOMER_ID          5000 non-null   int64  \n",
      " 1   x_customer_id        5000 non-null   float64\n",
      " 2   y_customer_id        5000 non-null   float64\n",
      " 3   mean_amount          5000 non-null   float64\n",
      " 4   std_amount           5000 non-null   float64\n",
      " 5   mean_nb_tx_per_day   5000 non-null   float64\n",
      " 6   available_terminals  5000 non-null   object \n",
      " 7   nb_terminals         5000 non-null   int64  \n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "customer_profiles_table_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "342a6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   TERMINAL_ID    10000 non-null  int64  \n",
      " 1   x_terminal_id  10000 non-null  float64\n",
      " 2   y_terminal_id  10000 non-null  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 234.5 KB\n"
     ]
    }
   ],
   "source": [
    "terminal_profiles_table_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c23cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2395956 entries, 0 to 2395955\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   TRANSACTION_ID   int64         \n",
      " 1   TX_DATETIME      datetime64[ns]\n",
      " 2   CUSTOMER_ID      object        \n",
      " 3   TERMINAL_ID      object        \n",
      " 4   TX_AMOUNT        float64       \n",
      " 5   TX_TIME_SECONDS  object        \n",
      " 6   TX_TIME_DAYS     object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
      "memory usage: 128.0+ MB\n"
     ]
    }
   ],
   "source": [
    "transactions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fbd65a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2022-12-12 17:07:44,053 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-12-12 17:07:46,999 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "        .appName(\"notebook-init\")\n",
    "        .master(\"yarn\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "691dd1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TRANSACTION_ID: long (nullable = true)\n",
      " |-- TX_DATETIME: timestamp (nullable = true)\n",
      " |-- CUSTOMER_ID: long (nullable = true)\n",
      " |-- TERMINAL_ID: long (nullable = true)\n",
      " |-- TX_AMOUNT: double (nullable = true)\n",
      " |-- TX_TIME_SECONDS: long (nullable = true)\n",
      " |-- TX_TIME_DAYS: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- TERMINAL_ID: long (nullable = true)\n",
      " |-- x_terminal_id: double (nullable = true)\n",
      " |-- y_terminal_id: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not supported type: <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1041\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1067\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, _infer_type(v), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.int64'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m terminal_profiles_table_sdf\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mcreateDataFrame(terminal_profiles_table_df) \n\u001b[1;32m      4\u001b[0m terminal_profiles_table_sdf\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m----> 5\u001b[0m customer_profiles_table_sdf\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomer_profiles_table_df\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      6\u001b[0m customer_profiles_table_sdf\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:603\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    600\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:300\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    299\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:630\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    632\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:451\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    448\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 451\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    453\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:383\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    381\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferring schema from dict is deprecated,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use pyspark.sql.Row instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:383\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    381\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferring schema from dict is deprecated,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use pyspark.sql.Row instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(_merge_type, (\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1069\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[0;32m-> 1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, _infer_type(v), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1069\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[0;32m-> 1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1032\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ArrayType(\u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayType(NullType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, array):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1043\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _infer_schema(obj)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "transactions_sdf=spark.createDataFrame(transactions_df) \n",
    "transactions_sdf.printSchema()\n",
    "terminal_profiles_table_sdf=spark.createDataFrame(terminal_profiles_table_df) \n",
    "terminal_profiles_table_sdf.printSchema()\n",
    "customer_profiles_table_sdf=spark.createDataFrame(customer_profiles_table_df) \n",
    "customer_profiles_table_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7296d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "2022-12-12 16:23:46,618 WARN scheduler.TaskSetManager: Stage 0 contains a task of very large size (45435 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not supported type: <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1041\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1067\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, _infer_type(v), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.int64'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m transactions_sdf\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mcreateDataFrame(transactions_df) \n\u001b[1;32m      2\u001b[0m transactions_sdf\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs:///user/otus/card-fraud-detection/transactions.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m terminal_profiles_table_sdf\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomer_profiles_table_df\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m terminal_profiles_table_sdf\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs:///user/otus/card-fraud-detection/customer_profiles_table.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m terminal_profiles_table_sdf\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mcreateDataFrame(terminal_profiles_table_df) \n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:603\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    600\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:300\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    299\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:630\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    632\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:451\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    448\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 451\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    453\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:383\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    381\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferring schema from dict is deprecated,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use pyspark.sql.Row instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:383\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    381\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferring schema from dict is deprecated,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use pyspark.sql.Row instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(_merge_type, (\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1069\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[0;32m-> 1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, _infer_type(v), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1069\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[0;32m-> 1069\u001b[0m fields \u001b[38;5;241m=\u001b[39m [StructField(k, \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1032\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ArrayType(\u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayType(NullType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, array):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/types.py:1043\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _infer_schema(obj)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "transactions_sdf.write.mode(\"append\").parquet(\"hdfs:///user/otus/card-fraud-detection/transactions.parquet\")\n",
    "terminal_profiles_table_sdf.write.mode(\"append\").parquet(\"hdfs:///user/otus/card-fraud-detection/terminal_profiles_table.parquet\")    \n",
    "customer_profiles_table_sdf.write.mode(\"append\").parquet(\"hdfs:///user/otus/card-fraud-detection/customer_profiles_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df=customer_profiles_table.groupby('CUSTOMER_ID').apply(lambda x : generate_transactions_table(x.iloc[0], nb_days=5)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d095230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea531123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
